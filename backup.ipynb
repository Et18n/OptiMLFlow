{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe519b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ Detected separator: ','\n",
      "✅ Loaded dataset shape: (50000, 7)\n",
      "✅ Columns: ['Manufacturer', 'Model', 'Engine size', 'Fuel type', 'Year of manufacture', 'Mileage', 'Price']\n",
      "\n",
      "✅ Selected Features (X): ['Manufacturer', 'Model', 'Engine size', 'Fuel type', 'Year of manufacture', 'Mileage']\n",
      "✅ Selected Target (y): Price\n",
      "\n",
      "🔍 Target Class Distribution:\n",
      "Price\n",
      "1610     0.00028\n",
      "805      0.00026\n",
      "1384     0.00026\n",
      "2033     0.00024\n",
      "1559     0.00024\n",
      "          ...   \n",
      "36387    0.00002\n",
      "8197     0.00002\n",
      "35058    0.00002\n",
      "8182     0.00002\n",
      "31112    0.00002\n",
      "Name: proportion, Length: 25045, dtype: float64\n",
      "✅ Features available for modeling: ['Manufacturer', 'Model', 'Engine size', 'Fuel type', 'Year of manufacture', 'Mileage']\n",
      "✅ Features available for modeling: ['Manufacturer', 'Model', 'Engine size', 'Fuel type', 'Year of manufacture', 'Mileage']\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 178\u001b[39m\n\u001b[32m    175\u001b[39m X_test_proc = np.asarray(X_test_proc)\n\u001b[32m    177\u001b[39m \u001b[38;5;66;03m# Apply feature selection only if feature space is large\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mX_train_proc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m > \u001b[32m100\u001b[39m:\n\u001b[32m    179\u001b[39m     feature_selector = SelectKBest(score_func=f_classif, k=\u001b[32m20\u001b[39m)\n\u001b[32m    180\u001b[39m     X_train_proc = feature_selector.fit_transform(X_train_proc, y_train)\n",
      "\u001b[31mIndexError\u001b[39m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from collections import Counter\n",
    "\n",
    "# -----------------------------\n",
    "# Dynamic dataset loader\n",
    "# -----------------------------\n",
    "def load_dataset_auto_sep(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        sample = f.read(2048)\n",
    "        f.seek(0)\n",
    "        dialect = csv.Sniffer().sniff(sample)\n",
    "        sep = dialect.delimiter\n",
    "        print(f\"ℹ️ Detected separator: '{sep}'\")\n",
    "    return pd.read_csv(path, sep=sep)\n",
    "\n",
    "# -----------------------------\n",
    "# Load dataset\n",
    "# -----------------------------\n",
    "data_path = 'Datasets/car_sales_data.csv'  # Change this dynamically if needed\n",
    "df = load_dataset_auto_sep(data_path)\n",
    "df = df.fillna(np.nan)\n",
    "\n",
    "print(f\"✅ Loaded dataset shape: {df.shape}\")\n",
    "print(f\"✅ Columns: {list(df.columns)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Automatically set features and target\n",
    "# -----------------------------\n",
    "X = df.iloc[:, :-1].copy()\n",
    "y = df.iloc[:, -1].copy()\n",
    "print(f\"\\n✅ Selected Features (X): {list(X.columns)}\")\n",
    "print(f\"✅ Selected Target (y): {y.name}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Safely drop likely ID columns\n",
    "# -----------------------------\n",
    "id_cols = [col for col in X.columns if X[col].nunique() == len(X) and X[col].dtype in [np.int64, object]]\n",
    "if id_cols:\n",
    "    remaining_cols = [col for col in X.columns if col not in id_cols]\n",
    "    if len(remaining_cols) == 0:\n",
    "        print(f\"⚠️ Skipping drop: Dropping {id_cols} would leave no features.\")\n",
    "    else:\n",
    "        print(f\"🚨 Dropping likely ID columns: {id_cols}\")\n",
    "        X = X.drop(columns=id_cols)\n",
    "\n",
    "# -----------------------------\n",
    "# Drop leakage columns (Regression only)\n",
    "# -----------------------------\n",
    "# if y.dtype in [np.float64, np.int64]:\n",
    "#     corr = X.corrwith(y).abs()\n",
    "#     leak_cols = corr[corr > 0.98].index.tolist()\n",
    "#     remaining_cols = [col for col in X.columns if col not in leak_cols]\n",
    "#     if leak_cols:\n",
    "#         if len(remaining_cols) == 0:\n",
    "#             print(f\"⚠️ Skipping drop: Dropping {leak_cols} would leave no features.\")\n",
    "#         else:\n",
    "#             print(f\"🚨 Dropping leakage columns: {leak_cols}\")\n",
    "#             X = X.drop(columns=leak_cols)\n",
    "\n",
    "# -----------------------------\n",
    "# Check for empty feature set\n",
    "# -----------------------------\n",
    "if X.shape[1] == 0:\n",
    "    print(\"❌ No features left after preprocessing steps. Restoring all columns except the target.\")\n",
    "    X = df.iloc[:, :-1].copy()\n",
    "\n",
    "# Dynamically split numeric pair columns\n",
    "for col in X.select_dtypes(include='object').columns:\n",
    "    if X[col].str.match(r'^\\d+/\\d+$').all():\n",
    "        print(f\"🔄 Splitting numeric pair column: {col}\")\n",
    "        split_df = X[col].str.split('/', expand=True)\n",
    "        X[f\"{col}_1\"] = pd.to_numeric(split_df[0], errors='coerce')\n",
    "        X[f\"{col}_2\"] = pd.to_numeric(split_df[1], errors='coerce')\n",
    "        X = X.drop(columns=[col])\n",
    "\n",
    "if X.shape[1] == 0:\n",
    "    print(\"❌ No features left after numeric pair splitting. Restoring all columns except the target.\")\n",
    "    X = df.iloc[:, :-1].copy()\n",
    "\n",
    "# -----------------------------\n",
    "# Detect numeric and categorical columns\n",
    "# -----------------------------\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=[object, 'category', 'bool']).columns.tolist()\n",
    "\n",
    "# -----------------------------\n",
    "# Check target distribution\n",
    "# -----------------------------\n",
    "print(\"\\n🔍 Target Class Distribution:\")\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "# -----------------------------\n",
    "# Detect high-cardinality categorical columns\n",
    "# -----------------------------\n",
    "high_card_cols = [col for col in cat_cols if X[col].nunique() > 50]\n",
    "low_card_cols = [col for col in cat_cols if col not in high_card_cols]\n",
    "if high_card_cols:\n",
    "    print(f\"⚡ High cardinality columns detected: {high_card_cols}. Using OrdinalEncoder for these.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Dynamic scaling choice\n",
    "# -----------------------------\n",
    "scaler_choice = 'StandardScaler' if (num_cols and X[num_cols].var().max() > 1) else 'MinMaxScaler'\n",
    "\n",
    "# -----------------------------\n",
    "# Train/test split\n",
    "# -----------------------------\n",
    "stratify_arg = y if (y.nunique() <= 20 and y.dtype != float) else None\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=stratify_arg\n",
    ")\n",
    "\n",
    "if len(X.columns) == 0:\n",
    "    raise ValueError(\"❌ No features available after preprocessing steps.\")\n",
    "\n",
    "print(\"✅ Features available for modeling:\", list(X.columns))\n",
    "\n",
    "# -----------------------------\n",
    "# Build preprocessing pipeline\n",
    "# -----------------------------\n",
    "transformers = []\n",
    "if num_cols:\n",
    "    num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler() if scaler_choice == 'StandardScaler' else MinMaxScaler())\n",
    "    ])\n",
    "    transformers.append(('num', num_pipeline, num_cols))\n",
    "\n",
    "if low_card_cols:\n",
    "    cat_low_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=True))  # Sparse for memory savings\n",
    "    ])\n",
    "    transformers.append(('cat_low', cat_low_pipeline, low_card_cols))\n",
    "\n",
    "if high_card_cols:\n",
    "    cat_high_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OrdinalEncoder())\n",
    "    ])\n",
    "    transformers.append(('cat_high', cat_high_pipeline, high_card_cols))\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers, remainder='drop')\n",
    "\n",
    "# -----------------------------\n",
    "# Fit and transform\n",
    "# -----------------------------\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "X_test_proc = preprocessor.transform(X_test)\n",
    "\n",
    "# Get feature names for the processed data\n",
    "feature_names = []\n",
    "if num_cols:\n",
    "    feature_names.extend(num_cols)\n",
    "if low_card_cols:\n",
    "    # Get feature names from OneHotEncoder\n",
    "    encoder = preprocessor.named_transformers_['cat_low'].named_steps['encoder']\n",
    "    feature_names.extend(encoder.get_feature_names_out(low_card_cols))\n",
    "if high_card_cols:\n",
    "    feature_names.extend(high_card_cols)\n",
    "\n",
    "# Convert to numpy array for modeling\n",
    "X_train_proc = np.asarray(X_train_proc)\n",
    "X_test_proc = np.asarray(X_test_proc)\n",
    "\n",
    "# Apply feature selection only if feature space is large\n",
    "if X_train_proc.shape[1] > 100:\n",
    "    feature_selector = SelectKBest(score_func=f_classif, k=20)\n",
    "    X_train_proc = feature_selector.fit_transform(X_train_proc, y_train)\n",
    "    X_test_proc = feature_selector.transform(X_test_proc)\n",
    "    print(\"Applied feature selection: Top 20 features kept.\")\n",
    "else:\n",
    "    print(\"Feature selection skipped (small feature space).\")\n",
    "\n",
    "print(\"\\nProcessed Features Sample:\")\n",
    "display(pd.DataFrame(X_train_proc, columns=feature_names).head())\n",
    "\n",
    "# -----------------------------\n",
    "# Save processed datasets\n",
    "# -----------------------------\n",
    "os.makedirs('output', exist_ok=True)\n",
    "train_df = pd.DataFrame(X_train_proc, columns=feature_names)\n",
    "train_df[y.name] = y_train.reset_index(drop=True)\n",
    "train_df.to_csv('output/train_processed.csv', index=False)\n",
    "\n",
    "test_df = pd.DataFrame(X_test_proc, columns=feature_names)\n",
    "test_df[y.name] = y_test.reset_index(drop=True)\n",
    "test_df.to_csv('output/test_processed.csv', index=False)\n",
    "\n",
    "print(\"\\n✅ Processed train/test datasets saved to /content/ (no automatic download).\")\n",
    "\n",
    "# -----------------------------\n",
    "# Model selection & training\n",
    "# -----------------------------\n",
    "models_classification = {\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(\n",
    "        n_estimators=100, max_depth=5, class_weight='balanced', random_state=42),\n",
    "    \"GradientBoostingClassifier\": GradientBoostingClassifier(\n",
    "        n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42),\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        max_iter=1000, solver='liblinear', random_state=42)\n",
    "}\n",
    "\n",
    "models_regression = {\n",
    "    \"RandomForestRegressor\": RandomForestRegressor(\n",
    "        n_estimators=100, max_depth=5, random_state=42),\n",
    "    \"GradientBoostingRegressor\": GradientBoostingRegressor(\n",
    "        n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42),\n",
    "    \"LinearRegression\": LinearRegression()\n",
    "}\n",
    "\n",
    "task_type = 'classification' if y.dtype == object or (y.nunique() <= 20 and y.dtype != float) else 'regression'\n",
    "models_to_train = models_classification if task_type == 'classification' else models_regression\n",
    "print(f\"\\nDetected model type: {task_type}\")\n",
    "print(\"Training all models to recommend the best one...\")\n",
    "\n",
    "cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=42) if task_type == 'classification' else 3\n",
    "\n",
    "performance = {}\n",
    "for name, model in models_to_train.items():\n",
    "    scores = cross_val_score(model, X_train_proc, y_train, cv=cv_strategy,\n",
    "                             scoring='accuracy' if task_type == 'classification' else 'r2')\n",
    "    performance[name] = scores.mean()\n",
    "\n",
    "best_model_name = max(performance, key=performance.get)\n",
    "print(\"\\nModel performance comparison:\")\n",
    "for k, v in performance.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(f\"\\n✅ Recommended model: {best_model_name} (best balance of speed and accuracy)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
